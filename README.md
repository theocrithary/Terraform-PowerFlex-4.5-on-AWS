# Powerflex AWS automation
This project will deploy APEX Block Storage (aka. PowerFlex) in AWS using Terraform based infrastructure as code and some manual steps that are required post deployment. 
This was tested using a RHEL 8 Linux server and may require small tweaks to the code depending on which OS you are using to run the Terraform deployment.

## Step 1: Pre-reqs

### Install Terraform
- https://developer.hashicorp.com/terraform/downloads
* e.g. RHEL
```
sudo yum install -y yum-utils
sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo
sudo yum -y install terraform
```

### Install AWS CLI
```
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
```

### Configure AWS creds
```
aws configure
```
provide access key ID and secret access key
enter default region name (e.g. eu-west-1)
enter default output format (e.g. json)

### Clone the repo
```
git clone https://github.com/theocrithary/Terraform-PowerFlex-4.5-on-AWS.git
```

### Navigate to the working directory
```
cd Terraform-PowerFlex-4.5-on-AWS/provision/deployments/vpn-mode
```

### Rename the vars.tf.example file to vars.tf
```
mv vars-example-tf vars.tf
```

### Edit the vars.tf file and replace any variables with your own environment variables
```
vi vars.tf
```

## Step 2: Run the Terraform deployment

### terraform init
```
terraform init
```

### terraform validate & plan
```
terraform validate && terraform plan
```

### terraform apply
```
terraform apply -auto-approve
```

### Confirm the deployment completed successfully, with no errors

## Step 3: Prepare all nodes with keys, root passwords and other required packages

### Copy the SSH keys and SSH to the installer instance
- Retrieve the IP address of the deployed installer instance by logging into the AWS console
- Copy the SSH key to the installer instance by using the ec2-user and key generated by the Terraform scripts
- SSH to the installer instance
```
cd ../../../keys/
scp -i "powerflex-denver-key" powerflex-denver-key ec2-user@172.26.2.42:/home/ec2-user/
ssh -i "powerflex-denver-key" ec2-user@172.26.2.42
```

### Copy the SSH key to all storage nodes
```
chmod 400 powerflex-denver-key
cp powerflex-denver-key .ssh/id_rsa
```
- Retrieve the IP addresses of all co-res storage nodes from the AWS console
- Copy the SSH key to each co-res storage node
```
scp .ssh/id_rsa ec2-user@172.26.2.53:.ssh/id_rsa
scp .ssh/id_rsa ec2-user@172.26.2.122:.ssh/id_rsa
scp .ssh/id_rsa ec2-user@172.26.2.160:.ssh/id_rsa
scp .ssh/id_rsa ec2-user@172.26.2.36:.ssh/id_rsa
scp .ssh/id_rsa ec2-user@172.26.2.118:.ssh/id_rsa
scp .ssh/id_rsa ec2-user@172.26.2.141:.ssh/id_rsa
```

### Install Kubectl CLI tool, eable root login and disable firewall on each PFMP node
```
ssh 172.26.2.53
curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
sudo sed -i 's/PermitRootLogin no/PermitRootLogin yes/g' /etc/ssh/sshd_config
sudo systemctl stop firewalld
sudo systemctl disable firewalld
sudo systemctl mask --now firewalld
sudo reboot

ssh 172.26.2.122
curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
sudo sed -i 's/PermitRootLogin no/PermitRootLogin yes/g' /etc/ssh/sshd_config
sudo sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/g' /etc/ssh/sshd_config
sudo systemctl restart sshd
sudo systemctl disable firewalld
sudo systemctl mask --now firewalld
sudo reboot

ssh 172.26.2.160
curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
sudo sed -i 's/PermitRootLogin no/PermitRootLogin yes/g' /etc/ssh/sshd_config
sudo systemctl stop firewalld
sudo systemctl disable firewalld
sudo systemctl mask --now firewalld
sudo reboot

```

## Step 4: Prepare to run the PFMP installer

### Prepare the JSON file for installer setup
- Retrieve the DNS of the load balancer from the AWS console
e.g. theocrithary-20240523T221731-4c32464c6a13470d.elb.eu-west-1.amazonaws.com
- Retrieve one of the IP's by resolving the DNS name
```
dig +short theocrithary-20240527T040224-9c54180ddbd88128.elb.eu-west-1.amazonaws.com | head -1
```
- Prepare the JSON file as per below and replace the "Nodes" hostname and IP with the co-res instances 1-3
```

{
    "Nodes":
    [
      {
        "hostname": "ip-172-26-2-53.eu-west-1.compute.internal",
        "ipaddress": "172.26.2.53"
      },
      {
        "hostname": "ip-172-26-2-122.eu-west-1.compute.internal",
        "ipaddress": "172.26.2.122"
      },
      {
        "hostname": "ip-172-26-2-160.eu-west-1.compute.internal",
        "ipaddress": "172.26.2.160"
      }
    ],
 
    "ClusterReservedIPPoolCIDR" : "10.42.0.0/23",
 
    "ServiceReservedIPPoolCIDR" : "10.43.0.0/23",
 
    "RoutableIPPoolCIDR" : [
	  {
	    "mgmt":"10.240.126.0/25"
	  }
    ],
    
    "PFMPHostname" : "172.26.2.188",
  
    "PFMPHostIP" : "172.26.2.188"
}
```

### Run the installer setup and install scripts
- Edit the JSON file and replace with the pre-prepared JSON file as per above
```
sudo vi /tmp/bundle/pfmp_deployments/PFMP*/PFMP_Installer/config/PFMP_Config.json
```
- Run the setup script
```
sudo /tmp/bundle/pfmp_deployments/PFMP*/PFMP_Installer/scripts/setup_installer.sh
```
- Copy the SSH key to the installer inventory directory
```
sudo cp ~/.ssh/id_rsa /tmp/bundle/pfmp_deployments/PFMP*/atlantic/inventory
```
- Run the installer script
```
sudo /tmp/bundle/pfmp_deployments/PFMP*/PFMP_Installer/scripts/install_PFMP.sh aws
```
- Answer the prompts as follows;
```
Are ssh keys used for authentication connecting to the cluster nodes[Y]?:y
Please enter the ssh username for the nodes specified in the PFMP_Config.json[root]:root
Are ssh keys the same for all the cluster nodes[Y]?:y
Please enter the location of the ssh key for the nodes specified in the PFMP_Config.json[id_rsa]:id_rsa
Are the nodes used for the PFMP cluster, co-res nodes [Y]?:y
```


### Open another SSH session on the installer server and watch the logs
```
ssh -i "powerflex-denver-key" ec2-user@172.26.2.42
tail -f /tmp/bundle/pfmp_deployments/PFMP*/atlantic/logs/bedrock.log
```

### Delete the installer VM

- Once the above script has completed and confirmed via the logs, you can then power off and delete the PFMP installer instance through the AWS console.

## Step 5: Login to PowerFlex Manager to complete setup

- Use a browser to open the PowerFlex Manager console; https://172.26.2.188

- Login with the default user account
```
admin / Admin123!
```

- Change the password when prompted

- Step through the Initial Config Wizard and select "I want to deploy a new instance of PowerFlex"

- Upload the compliance bundle (e.g. https://pflex-packages.s3.eu-west-1.amazonaws.com/pflex-45/Software_Only_Complete_4.5.2_135/PowerFlex_Software_4.5.2.0_135_r1.zip) 
      - requires a CIFs/SMB file share to host the file or a web server such as AWS S3 with a public URL

- The package download will take a few mins to complete, but will raise a critical warning. Action it by allowing unsigned package.

- Upload the compatibility management version file (e.g. https://pflex-packages.s3.eu-west-1.amazonaws.com/pflex-45/Software_Only_Complete_4.5.2_135/cm-20240314-01.gpg)
     - Settings -> compatibility management -> upload file

- Go back to the installation configuration wizard page by navigating to the ? in the top right and clicking on 'getting started'

- Configure the networks
```
      - Define networks -> Define
      - Name: powerflex-az1
      - Network Type: General Purpose LAN
      - VLAN ID: 1
      - Subnet: 172.26.2.0
      - Subnet Mask: 255.255.255.192
      - Gateway: blank
      - Primary DNS: blank
      - Secondary DNS: blank
      - DNS Suffix: blank
      - IP Address Range
      - Role: Server or Client
      - Starting IP: 172.26.2.53
      - Ending IP: 172.26.2.53
      - Starting IP: 172.26.2.36
      - Ending IP: 172.26.2.36

      - Define networks -> Define
      - Name: powerflex-az2
      - Network Type: General Purpose LAN
      - VLAN ID: 1
      - Subnet: 172.26.2.64
      - Subnet Mask: 255.255.255.192
      - Gateway: blank
      - Primary DNS: blank
      - Secondary DNS: blank
      - DNS Suffix: blank
      - IP Address Range
      - Role: Server or Client
      - Starting IP: 172.26.2.122
      - Ending IP: 172.26.2.122
      - Starting IP: 172.26.2.118
      - Ending IP: 172.26.2.118

      - Define networks -> Define
      - Name: powerflex-az3
      - Network Type: General Purpose LAN
      - VLAN ID: 1
      - Subnet: 172.26.2.128
      - Subnet Mask: 255.255.255.192
      - Gateway: blank
      - Primary DNS: blank
      - Secondary DNS: blank
      - DNS Suffix: blank
      - IP Address Range
      - Role: Server or Client
      - Starting IP: 172.26.2.160
      - Ending IP: 172.26.2.160
      - Starting IP: 172.26.2.141
      - Ending IP: 172.26.2.141
```

## Step 6: Install PowerFlex software on SDS storage nodes
- Prepare a CSV file with the correct root password, IP's of all nodes and verify the storage devices to be used
- Copy the required software from the "Complete Software" download bundle obtained from the Dell support site
- Be sure to copy the rpm's that match your instance OS version (e.g. PowerFlex_4.5.2000.135_Complete_Core_SW\PowerFlex_4.5.2000.135_SLES15.4.zip matches SLES 15.4)
- Copy the following packages;
      - EMC-ScaleIO-activemq-5.xx.xxxx.noarch.rpm
      - EMC-ScaleIO-mdm-4.5xx.xxx.sles15.4.x86_64.rpm
      - EMC-ScaleIO-sds-4.5xx.xxx.sles15.4.x86_64.rpm
      - EMC-ScaleIO-sdt-4.5xx.xxx.sles15.4.x86_64.rpm
      - EMC-ScaleIO-lia-4.5xx.xxx.sles15.4.x86_64.rpm
      - EMC-ScaleIO-sdr-4.5xx.xxx.sles15.4.x86_64.rpm
- Return to the PowerFlex Manager UI
- Go to the 'installation configuration wizard' page by navigating to the ? in the top right and clicking on 'getting started'
- Click 'Deploy With Installation File'
- Click 'Browse' and upload your RPM packages as prepared earlier
- After clicking 'Next', you will be at the CSV section
- Click 'Browse' and upload the CSV prepared earlier
- Click 'I accept the terms of the End User License Agreement (mandatory)'
- Click 'Next' and 'Go to Events Page' to track the installation progress
- Confirm the system is deployed successfully by navigating through each of the PowerFlex components and confirming their health status
- There may be an error with the storage pool spare capacity being less than the configured fault set. To resolve this error, change the default setting of the storage pool spare capacity from 35 to 50%


## Step 7: Install the SDC client on a Linux host

### RHEL
- Obtain the following files from the complete SW package and transfer to Linux host
      - RPM-GPG-KEY-ScaleIO_4.5.2000.135
      - EMC-ScaleIO-sdc-4.5-2000.135.el8.x86_64.rpm

- Run the following commands to install the SDC client and connect it to the MDM;
```
rpm --import RPM-GPG-KEY-ScaleIO_4.5.2000.135
MDM_IP=172.26.2.36 rpm -i EMC-ScaleIO-sdc-4.5-2000.135.el8.x86_64.rpm
service scini status
systemctl status scini
```
- Check that the MDM has been configured correctly
```
/opt/emc/scaleio/sdc/bin/drv_cfg --query_mdms
```
- Check if there are any existing volumes mapped to this host
```
/opt/emc/scaleio/sdc/bin/drv_cfg --query_vols
```

## Step 8: Create a volume and map it to the SDC client
- Login to PowerFlex Manager console
- Navigate to the 'Block' storage tab and select 'Hosts'
- Observe the newly added SDC client host IP
- Navigate to the 'Block' tab and select 'Volumes'
- Click '+ Create Volume'
- Provide a name, size and select a storage pool, then click create
- After the volume is created, click 'map' when you see the popup in the bottom left corner of the console
- Select the SDC client from the host list and click 'map'

## Step 9: Test the volume
- SSH back into the SDC client
- Scan for any new volumes
```
/opt/emc/scaleio/sdc/bin/drv_cfg --rescan
```
- Confirm the volume was connected
```
/opt/emc/scaleio/sdc/bin/drv_cfg --query_vols
```
- Check the new block device and take note of the device name (e.g. scinia)
```
lsblk -f
```
- Format the device with a filesystem (e.g. EXT4)
```
mkfs -t ext4 /dev/scinia
```
- Create a new directory to mount the device
```
mkdir /data1
```
- Mount the device to the new directory
```
mount /dev/scinia /data1
```
- Change to the path and create a test file to confirm read/write access to the volume
```
cd /data1
touch testfile
```